<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Recursively Scraping A Blog With Scrapy</title>
    <link rel="stylesheet" href="/stylesheets/styles.css">
    <link rel="stylesheet" href="/stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <link href='http://fonts.googleapis.com/css?family=Abel' rel='stylesheet' type='text/css'>    
    <link href='http://fonts.googleapis.com/css?family=Life+Savers' rel='stylesheet' type='text/css'>
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <style>
      body{
        color: #404040;
        padding:0;
        margin: 0;
      }
      .wrapper {
        margin: 0 0;
        padding: 50px;
      }

      a .post{
        margin-bottom: 5px;
        font-size: 18px;
      }

      time{
        font-family: 'Abel', sans-serif;
        font-size: 0.9em;
        display: block;
        color: #888;
        margin-bottom: 18px;
      }

      footer p{
        margin-bottom: 3px;
      }

      footer{
        bottom: 30px;
      }

      .nav p{
        margin-right: 7px;
        float: left;
      }
      .nav{
        float: left;
      }

      #cright{
        margin-top: 10px;
      }

      .nav-ele{
        padding: 1px 4px 2px;
        -webkit-border-radius: 3px;
        -moz-border-radius: 3px;
        border-radius: 3px;
        background-color: #505050;
      }

      .nav-ele a{
        font-weight: bold;
        color: white;
      }

      .nav-ele a:hover{
        color: #ccc;
      }

      .archives{
        margin-top: 20px;
        margin-bottom: 8px;
        margin-left: -20px;
      }

      .topbar-fixed{
        top: 0;
        right: 0;
        left: 0;
        z-index: 1030;
        margin-bottom: 0;
        position: fixed;
      }

      .topbar-inner{
        min-height: 20px;
        width:100%;
        background-color: #D44413;
        box-shadow: 0 1px 3px rgba(0, 0, 0, .5), inset 0 -1px 0 rgba(0, 0, 0, .1);
      }
      
      .quote-sidebar{
      font-weight:bold;
      }
      .quote-sidebar .quote{
      margin-bottom: 5px;
      padding-top:40px;
      font-family: 'Life Savers', cursive;
      }
      .quote-sidebar .author{
      float:right;
      margin-right: 40px;
      }
.pub-list{
list-style: none;
margin-left: 0;
}

.pu-list li{
margin-top: 10px;
}

    </style>
  </head>
  <body>
    <!--div class="topbar-fixed">
      <div class="topbar-inner" id="css-animation">
      </div>
    </div-->
    <div class="wrapper">
      <header>
        <h1>Hey, I'm Milinda</h1>
        <p>A Father and a Computer Science PhD Student at the <a href="http://www.soic.indiana.edu/">Indiana University</a>. I am also a committer and a PMC member for <a href="http://axis.apache.org/">Apache Axis</a> and <a href="http://ode.apache.org/">Apache ODE</a> projects.</p>
        <p>Get in touch with me on <a href="http://twitter.com/milindalakmal">Twitter</a>, <a href="http://www.facebook.com/milindapathirage">Facebook</a> or plain old-fashioned <a href="mailto:milinda.pathirage@gmail.com">email</a>.</p>
        <div class="nav">
          <p class="nav-ele"><a href="/">Home</a></p>
          <p class="nav-ele"><a href="/archive.html">Archives</a></p>
          <p class="nav-ele"><a href="/gsoc.html">GSOC 2011</a></p>
        </div>
	
	<div style="clear:both;" class="quote-sidebar">
	  <p class="quote">Don't ignore your dreams; don't work too much; say what you think; cultivate friendships; be happy.</p>
	  <p class="author">- <a href="http://paulgraham.com/todo.html" target="_blank">Paul Graham</a></p>
	</div>
      </header>
      <section>
        <h1>Recursively Scraping A Blog With Scrapy</h1>
<p><a href="http://scrapy.org">Scrapy</a> is a web crawling and scraping framework
written in python. The framework is really simple to understand and easy
to get started with. If you know little bit of Python, you should be
able to build your own web scraper within few minutes. In this post I'm
going to describe how you can use Scrapy to build recursive blog
crawler. Building a recursive scraper using Scrapy is pretty simple, yet
it's getting started guide doesn't help people who are unfamiliar with
the framework to write a recursive scraper.</p>

<p>I'm going to use <a href="http://blog.scrapy.org">blog.scrapy.org</a> as the target
blog and techniques I am discussing will work on other sites as well
with simple changes to information extraction queries. If you are new to
Scrapy it's required to read the <a href="http://doc.scrapy.org/en/latest/intro/tutorial.html">Scrapy tutorial</a> first.
I also assume that you have Scrapy <a href="http://doc.scrapy.org/en/latest/intro/install.html">installed</a> in your machine.</p>

<p>Let's create a Scrapy project first using following command:</p>

<div class="highlight"><pre><code class="bash">scrapy startproject scrapy_sample
</code></pre>
</div>


<h2>Defining the Scrapy Item</h2>

<p>Next step is to define the Item which is the container Scrapy spider
used to store the scraped data. I'm going to extract the blog post link,
post title and text content of the post. So my Item definition will look like
following.</p>

<div class="highlight"><pre><code class="python"><span class="kn">from</span> <span class="nn">scrapy.item</span> <span class="kn">import</span> <span class="n">Item</span><span class="p">,</span> <span class="n">Field</span>

<span class="k">class</span> <span class="nc">ScrapySampleItem</span><span class="p">(</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
    <span class="n">link</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
    <span class="n">content</span> <span class="o">=</span> <span class="n">Field</span><span class="p">()</span>
</code></pre>
</div>


<h2>Implementing the spider</h2>

<p>Our spider will define initial URL to download content from, how to
follow pagination links and how to extract blog posts in a page and
creating items from the posts.</p>

<p>Your spider class must be a subclass of <a href="http://doc.scrapy.org/en/latest/topics/spiders.html#scrapy.spider.BaseSpider">scrapy.spider.BaseSpider</a> and
you need to define three main mandatory attributes.
- <strong>name</strong> : Unique identifier for the spider
- <strong>start_urls</strong> : List of URLs to begin crawling
- <strong>parse()</strong> : Method which will be called with the downloaded
  <a href="http://doc.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response">Response</a> object for each start URL. Code related to parsing and data
  extraction will go under this.</p>

<p>Our spider implementation will look like following:</p>

<div class="highlight"><pre><code class="python"><span class="kn">from</span> <span class="nn">scrapy.spider</span> <span class="kn">import</span> <span class="n">BaseSpider</span>
<span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">HtmlXPathSelector</span>
<span class="kn">from</span> <span class="nn">scrapy.http.request</span> <span class="kn">import</span> <span class="n">Request</span>
<span class="kn">from</span> <span class="nn">scrapy_sample.items</span> <span class="kn">import</span> <span class="n">ScrapySampleItem</span>

<span class="k">class</span> <span class="nc">ScrapyOrgSpider</span><span class="p">(</span><span class="n">BaseSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s">&quot;scrapy&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;scrapy.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;http://blog.scrapy.org/&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">hxs</span> <span class="o">=</span> <span class="n">HtmlXPathSelector</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

        <span class="n">next_page</span> <span class="o">=</span>
            <span class="n">hxs</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&quot;//div[@class=&#39;pagination&#39;]/a[@class=&#39;next_page&#39;]/@href&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="ow">not</span> <span class="n">next_page</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">Request</span><span class="p">(</span><span class="n">next_page</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

        <span class="n">posts</span> <span class="o">=</span> <span class="n">hxs</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&quot;//div[@class=&#39;post&#39;]&quot;</span><span class="p">)</span>
        <span class="n">items</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">posts</span><span class="p">:</span>
            <span class="n">item</span> <span class="o">=</span> <span class="n">ScrapySampleItem</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&quot;title&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&quot;div[@class=&#39;bodytext&#39;]/h2/a/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&quot;link&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&quot;div[@class=&#39;bodytext&#39;]/h2/a/@href&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">item</span><span class="p">[</span><span class="s">&quot;content&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&quot;div[@class=&#39;bodytext&#39;]/p/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
            <span class="n">items</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">item</span>
</code></pre>
</div>


<p>First we create <a href="http://doc.scrapy.org/en/latest/topics/selectors.html#scrapy.selector.HtmlXPathSelector">HtmlXpathSelector</a> giving the response object.
This will allow you to select elements in response HTML using <a href="http://www.w3.org/TR/xpath">XPath</a> <a href="http://doc.scrapy.org/en/latest/topics/selectors.html#topics-selectors">selectors</a>. Then we extract the link to the next page of the blog using <strong>"//div[@class='pagination']/a[@class='next_page']/@href"</strong> XPath selector and selector you need to use in your code will depend on the web site you are going to crawl. Once we get the URL of the next page we check whether there are any URLs in the retirned list by selector, because last page will not have a next page link and Scrapy will throw a error when tried to go to empty URL while in the last page of the crawl. Main trick here is we are returning a python generator for the recursive call. You can learn more about reason behind this from this <a href="http://stackoverflow.com/questions/231767/the-python-yield-keyword-explained">stackoverflow conversation</a>. Last thing we are doing inside our parse method is extracting blog posts in the current page and creating list of Scrapy Items for blog posts.</p>

<h2>Running the scraper</h2>

<p>Now you can execute your scraper by  running following command while in
the root directory of your Scrapy project.</p>

<div class="highlight"><pre><code class="bash">scrapy crawl scrapy
</code></pre>
</div>


<p>Scrapy allows you to save the scraped items into a JSON formatted file.
All you have to do is add <strong>-o filename.json -t json</strong> option to previous
crawl command. This will save the scraped items into a JSON file with
the given name.</p>

<p>You can find more information about Scrapy from <a href="http://doc.scrapy.org/en/latest/index.html#section-basics">here</a>. I strongly recommend you to read the full documentation if you like to dig deeper into Scrapy.</p>

<div class="alert alert-success">
<strong>Source code for the sample can be found <a href="https://github.com/milinda/Scrapy-Sample">here</a>.</strong>
</div>


<div id="comments">
        <div id="disqus_thread"></div>
        <script type="text/javascript">
            /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
            var disqus_shortname = 'milindablog'; // required: replace example with your forum shortname

            /* * * DON'T EDIT BELOW THIS LINE * * */
            (function () {
                var dsq = document.createElement('script');
                dsq.type = 'text/javascript';
                dsq.async = true;
                dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
                (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by
            Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>

      </section>
      <footer>
            <p id="cright">Copyright Â© 2012 Milinda Pathirage</p>
            <p><small>Based on Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-2420520-12']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
  </body>
</html>
