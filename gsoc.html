---
layout: layout
title: GSOC 2012 Project Proposal
---
<style>
    .container ul{
        list-style-type:circle;
    }
</style>
<h1>Cloud Bursting Support for Airavata Computational Workflows</h1>
<section id="introduction">
<p>Apache Airavata provides capabilities to construct, execute and monitor computational workflows with built in
support for executing compute intensive applications on grid computing resources. With the increasing interest
on hybrid execution of compute intensive tasks on grids and clouds, solving the problem of selecting which computing
resource to use has become one of the major problems in computational workflows. </p>

<p>Apache Hadoop is a framework that allows distributed processing of large data sets across clusters of computers
using MapReduce programming model and domain scientists from all over the world use it for various domains
including human analyzing , text classification, data mining and machine learning. Main aim of this project is to
support cloud bursting for Hadoop based computational tasks. </p>

<p> This project will develop algorithms and software components required for efficient selection of computing resources(to run Hadoop jobs) based on the distribution of data to be processed, the tolerable latency on shared batch queued grid clusters, and the nature and size of the problem to be solved.</p>
</section>
<section id="problem">
<h2>Problems</h2>
<p>To efficiently select the computing resource, we first need to define on which basis we are going to select. Following criterion will be considered and still there are lot research to be done on these criterion to fine tune them and identify new criterion to select the computing resources.</p>
<ul>
    <li>Distribution of data be processed</li>
    <li>Tolerable latency on shared batch queued grid clusters</li>
    <li>Nature and size of the problem to be solved</li>
</ul>
<p>For large data analysis problems we may need move part of the problem to cloud or move simple computational jobs to local clusters from the cloud. Especially for applications which reduce the data as they proceed in the graph, they better fit for local MapReduce executions residing on Hadoop based file systems. Exploring the solution to this problem will be interesting and this will open up new research problems on cloud bursting MapReduce jobs partially.</p>
</section>
<section id="solution">
<h2>Benifits To Airavata</h2>
<p>Primary target of Airavata project is to build science gateways using computational resources from various disciplines. The initial targeted set of gateways include projects supporting research and education in chemistry, life sciences, biophysics, environmental sciences, geosciences astronomy and nuclear physics.</p>

<p>Most of the problems in above mentioned scientific domains involve huge data processing tasks and Hadoop is becoming the de facto technology for implementing compute intensive data processing tasks. This project will allow Airavata workflows to efficiently schedule Hadoop based jobs on various computing resource by reducing the overheads involved in data movement, and waiting in queues to get picked up by batch job execution systems. This project will make resource selection transparent and will reduce the complexity of setting up infrastructure required to run the computational tasks. </p>

<p>In addition to the direct benefits to Airavata project and the scientific community this project will open up new research directions on moving Hadoop jobs from and to cloud for efficient utilization of resources. </p>
</section>
<section id="deliverables">
<h2>Implementation</h2>
<p>Resource selection component will implemented as a module for Airavata Workflow Interpreter. The module will implement support for heuristics based resource selection, automated deployment of Hadoop on computational resources and automated migration of Hadoop jobs to/from clouds(grids) to/from local. As the project goes on I am planing to evaluate feasibility of different migration approaches and when to and where to migrate will change depends on the experience I get during initial phase of the project.</p>

<p>Apache Whirr will be used for cloud integration and selection of libraries and components to interact with grids and local clusters will be decided based on the requirements. Extending Apache Whirr to support grid computing resources will be considered based on the availability of libraries to interact and do automated deployments of Hadoop.</p>

<p>Implementation will be divided into three phases.</p>
<h3>Phase 1</h3>
<p>Extend Airavata workflow providers to interact with commercial cloud platforms for execution of Hadoop based jobs. In this phase I’ll use Apache Whirr as a high level API for cloud platforms and integrate it with Airavata workflow interpreter to setup and execute Hadoop jobs on commercial cloud platforms. This will be a great value addition to Airavata due to the fact that Apache Whirr support multiple cloud platforms and this will allow Airavata to utilize major cloud platforms with least effort for computing resource management and deployment.</p>
<h3>Phase 2</h3>
<p>In this phase I’ll focus on implementing support scaling out Hadoop jobs running on local clusters to cloud based on the saturation level of local resources. In addition to commercial cloud platforms, support for computational grids will also be added to this module. This scaling technique is often referred to as Cloud bursting and this project will not only enable cloud bursting for Hadoop based jobs, it’ll open up the path and give us the necessary experience to implement cloud bursting for other computational jobs supported by Airavata.</p>
<h3>Phase 3</h3>
<p>In this phase I’ll focus on exploring the ways to determine which computing resource to select for the task at hand. I’m planning to implement heuristics based algorithm to support efficient selection of computing resources. Also I’m planning to do the testing and evaluation of the cloud bursting module implemented in the previous phases in this last phase. Plan is to use real world use case from a scientific domain as the test workflow.</p>
</section>
<section id="time-line">
<h2>Time Line</h2>
<div>
    <ul>
        <li><strong>April 23 - May 10</strong> - Exploring Airavata project and getting familiarized with concepts in Airavata project.</li>
        <li><strong>May 11 - May 31</strong> - First phase of the project. I’ll mainly focus on implementing support for cloud platforms based on Apache Whirr.</li>
        <li><strong>June 1 - June 30</strong> - Second phase of the project. Implementing support computational grids and Implementing support for partial migration of Hadoop jobs to and from clouds(grids).</li>
        <li><strong>August 1 - August 20</strong> - Third phase of the project. Exploring heuristics based algorithms for efficient selection of computing resources, testing and evaluation of the modules developed using real world use case.</li>
    </ul>
</div>
</section>
<section id="resources">
<h2>Resources</h2>
<ul>
    <li><a href="http://incubator.apache.org/airavata/">http://incubator.apache.org/airavata/</a></li>
    <li><a href="http://www.cse.ohio-state.edu/~bicer/research/papers/cluster2011.pdf">http://www.cse.ohio-state.edu/~bicer/research/papers/cluster2011.pdf</a></li>
    <li><a href="http://whirr.apache.org/">http://whirr.apache.org/</a></li>
    <li><a href="http://www.xyratex.com/pdfs/whitepapers/Xyratex_white_paper_MapReduce_1-4.pdf">http://www.xyratex.com/pdfs/whitepapers/Xyratex_white_paper_MapReduce_1-4.pdf<a></li>
            <li><a href="http://bowtie-bio.sourceforge.net/crossbow/index.shtml">http://bowtie-bio.sourceforge.net/crossbow/index.shtml</a></li>

</ul>
</section>


